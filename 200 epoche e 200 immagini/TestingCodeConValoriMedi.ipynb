{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzDMn5o8Z8z2","executionInfo":{"status":"ok","timestamp":1738849847072,"user_tz":-60,"elapsed":1102,"user":{"displayName":"SIMONE VENERUSO","userId":"17036499561198285880"}},"outputId":"6f8b4ac4-0b01-4bb4-99a4-f15c810f2923"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import imageio.v2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import tensorflow as tf\n","from keras import layers, models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from skimage.metrics import structural_similarity as ssim\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["test_data_path = '/content/drive/MyDrive/Progetto Compressione dati/Dataset RGB/100 images'\n","num_lightfields = 25\n","num_cameras_x = 2\n","num_cameras_y = 2\n","height, width = 1024,1024\n","MODEL_SAVE_PATH = '/content/drive/MyDrive/Progetto Compressione dati/200 epoche e 200 immagini/compressione1024x1024_200.keras'"],"metadata":{"id":"D308oVEDbtWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LightfieldDataset(Dataset):\n","    def __init__(self, data_path, num_lightfields, num_cameras_x, num_cameras_y):\n","        self.num_lightfields = num_lightfields\n","        self.num_cameras_x = num_cameras_x\n","        self.num_cameras_y = num_cameras_y\n","        self.data_path = data_path\n","\n","    def __len__(self):\n","        return self.num_lightfields\n","\n","    def __getitem__(self, idx):\n","        lightfield = torch.zeros(3, height, width, self.num_cameras_y, self.num_cameras_x)\n","        for x in range(self.num_cameras_x):\n","            for y in range(self.num_cameras_y):\n","                img_path = os.path.join(self.data_path, f'image{idx}_x{x}_y{y}.png')\n","                if not os.path.exists(img_path):\n","                    raise FileNotFoundError(f\"File non trovato: {img_path}\")\n","                view = Image.open(img_path).resize((width, height))\n","                view = torch.Tensor(np.array(view)).permute(2, 0, 1) / 255.0\n","                lightfield[:, :, :, y, x] = view\n","        lightfield = lightfield.view(3, height, width, -1).permute(3, 1, 2, 0)\n","        return lightfield"],"metadata":{"id":"1-KJSHCcb0Dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_reconstruction_and_compression(model, dataloader, num_images=4):\n","    encoder = models.Model(inputs=model.input, outputs=model.get_layer(index=len(model.layers)//2).output)\n","    decoder = models.Model(inputs=model.get_layer(index=len(model.layers)//2).output, outputs=model.output)\n","\n","    fig, axs = plt.subplots(3, num_images, figsize=(15, 15))\n","    data_iter = iter(dataloader)\n","    batch = next(data_iter)\n","    batch_np = batch.numpy()\n","\n","    compressed = encoder.predict(batch_np)\n","    reconstructed = decoder.predict(compressed)\n","\n","    print(f\"Dimensione delle immagini di partenza: {batch_np.shape}\")\n","    print(f\"Dimensione delle immagini compresse: {compressed.shape}\")\n","    print(f\"Dimensione delle immagini decompresse: {reconstructed.shape}\")\n","\n","    original_size_kb = batch_np.nbytes / 1024\n","    compressed_size_kb = compressed.nbytes / 1024\n","    reconstructed_size_kb = reconstructed.nbytes / 1024\n","    compression_ratio = (((compressed_size_kb * 100) / original_size_kb) - 100) * -1\n","\n","    num_views = num_cameras_x * num_cameras_y\n","    single_image_original_size_kb = original_size_kb / num_views\n","    single_image_compressed_size_kb = compressed_size_kb / num_views\n","    single_image_reconstructed_size_kb = reconstructed_size_kb / num_views\n","\n","    # Stampa dei risultati\n","    print(f\"Peso della Light Field Image originale: {original_size_kb:.2f} KB\")\n","    print(f\"Peso della Light Field Image compressa: {compressed_size_kb:.2f} KB\")\n","    print(f\"Peso della Light Field Image ricostruita: {reconstructed_size_kb:.2f} KB\")\n","    print(f\"Rapporto di compressione Light Field Image: {compression_ratio:.2f}%\")\n","\n","    print(f\"Peso della singola immagine originale: {single_image_original_size_kb:.2f} KB\")\n","    print(f\"Peso della singola immagine compressa: {single_image_compressed_size_kb:.2f} KB\")\n","    print(f\"Peso della singola immagine ricostruita: {single_image_reconstructed_size_kb:.2f} KB\\n\")\n","\n","    for i in range(num_images):\n","        # Immagini originali\n","        axs[0, i].imshow(batch_np[0, i])\n","        axs[0, i].set_title(\"Originale\")\n","        axs[0, i].axis(\"off\")\n","\n","        compressed_view = np.mean(compressed[0, i], axis=-1)\n","        axs[1, i].imshow(compressed_view, cmap='viridis')\n","        axs[1, i].set_title(\"Compressa\")\n","        axs[1, i].axis(\"off\")\n","\n","        axs[2, i].imshow(np.clip(reconstructed[0, i], 0, 1))\n","        axs[2, i].set_title(\"Ricostruita\")\n","        axs[2, i].axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"wVaFOR-Ab7TA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_dataset_metrics(model, dataloader):\n","    encoder = models.Model(inputs=model.input, outputs=model.get_layer(index=len(model.layers)//2).output)\n","    decoder = models.Model(inputs=model.get_layer(index=len(model.layers)//2).output, outputs=model.output)\n","\n","    total_original_size = 0\n","    total_compressed_size = 0\n","    total_reconstructed_size = 0\n","    total_psnr = 0\n","    total_ssim = 0\n","    num_images_processed = 0\n","\n","    for batch in dataloader:\n","        batch_np = batch.numpy()\n","        compressed_batch = encoder.predict(batch_np)\n","        reconstructed_batch = decoder.predict(compressed_batch)\n","\n","        total_original_size += batch_np.nbytes\n","        total_compressed_size += compressed_batch.nbytes\n","        total_reconstructed_size += reconstructed_batch.nbytes\n","\n","        for i in range(batch_np.shape[1]): # Iterate over images in the batch (batch_size=1, so batch_np.shape[0]=1, batch_np.shape[1] is num_images per lightfield)\n","            original_image = batch_np[0, i]\n","            reconstructed_image = reconstructed_batch[0, i]\n","\n","            psnr_value = psnr(original_image, reconstructed_image)\n","            ssim_value = ssim(\n","                original_image,\n","                reconstructed_image,\n","                win_size=5,\n","                multichannel=True,\n","                channel_axis=-1,\n","                data_range=1.0\n","            )\n","            total_psnr += psnr_value\n","            total_ssim += ssim_value\n","            num_images_processed += 1\n","\n","    avg_psnr = total_psnr / num_images_processed\n","    avg_ssim = total_ssim / num_images_processed\n","    compression_ratio = (((total_compressed_size * 100) / total_original_size) - 100) * -1\n","\n","    print(f\"------------------------Dataset Metrics------------------------\")\n","    print(f\"Dimensione totale immagini originali: {total_original_size / 1024:.2f} KB\")\n","    print(f\"Dimensione totale immagini compresse: {total_compressed_size / 1024:.2f} KB\")\n","    print(f\"Dimensione totale immagini ricostruite: {total_reconstructed_size / 1024:.2f} KB\")\n","    print(f\"Rapporto di compressione medio sul dataset: {compression_ratio:.2f}%\")\n","    print(f\"PSNR medio sul dataset: {avg_psnr:.2f} dB\")\n","    print(f\"SSIM medio sul dataset: {avg_ssim:.4f}\")\n","    print(f\"---------------------------------------------------------------\")"],"metadata":{"id":"FoRSLY9XcAlg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_autoencoder = models.load_model(MODEL_SAVE_PATH)\n","test_dataset = LightfieldDataset(data_path=test_data_path, num_lightfields=num_lightfields, num_cameras_x=num_cameras_x, num_cameras_y=num_cameras_y)\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"],"metadata":{"id":"V03SeDnIcFVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_reconstruction_and_compression(loaded_autoencoder, test_loader, num_images=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZMti_w_dcIdr","executionInfo":{"status":"ok","timestamp":1738849887885,"user_tz":-60,"elapsed":33820,"user":{"displayName":"SIMONE VENERUSO","userId":"17036499561198285880"}},"outputId":"05b0988f-b894-49df-82b1-a6ef38ab8e75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calculate_dataset_metrics(loaded_autoencoder, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JolvcHRmcLNb","executionInfo":{"status":"ok","timestamp":1738850070535,"user_tz":-60,"elapsed":182619,"user":{"displayName":"SIMONE VENERUSO","userId":"17036499561198285880"}},"outputId":"fb1de8eb-47fd-467b-cc18-f2ee0315b136"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step\n","------------------------Dataset Metrics------------------------\n","Dimensione totale immagini originali: 1228800.00 KB\n","Dimensione totale immagini compresse: 19200.00 KB\n","Dimensione totale immagini ricostruite: 1228800.00 KB\n","Rapporto di compressione medio sul dataset: 98.44%\n","PSNR medio sul dataset: 25.08 dB\n","SSIM medio sul dataset: 0.6559\n","---------------------------------------------------------------\n"]}]}]}